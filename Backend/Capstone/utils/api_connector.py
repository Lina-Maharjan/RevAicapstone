import requests
import re
import time
from typing import List
from config.settings import settings
import logging

logger = logging.getLogger(__name__)

def extract_asin_from_url(url: str) -> str:
    """Extract ASIN from Amazon URL"""
    patterns = [
        r"/dp/([A-Z0-9]{10})",
        r"/gp/product/([A-Z0-9]{10})",
        r"asin=([A-Z0-9]{10})",
        r"/product/([A-Z0-9]{10})"
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    
    raise ValueError("Invalid Amazon URL. Could not extract ASIN.")

def fetch_reviews_from_amazon(asin: str, max_pages: int = 3) -> List[str]:
    """Fetch reviews from Amazon using RapidAPI - Fixed to handle actual API response format"""
    all_reviews = []
    
    for page in range(1, max_pages + 1):
        # Using the exact URL format from your RapidAPI code
        url = "https://real-time-amazon-data.p.rapidapi.com/product-reviews"
        
        querystring = {
            "asin": asin,
            "country": "US", 
            "page": str(page)
        }
        
        # Using the exact header format from your RapidAPI code
        headers = {
            "x-rapidapi-key": settings.RAPIDAPI_KEY,
            "x-rapidapi-host": settings.RAPIDAPI_HOST
        }
        
        try:
            logger.info(f"üîç Fetching page {page} for ASIN: {asin}")
            
            # Retry logic for 503 errors (service overload)
            max_retries = 3
            retry_delay = 2  # seconds
            
            for attempt in range(max_retries):
                response = requests.get(url, headers=headers, params=querystring, timeout=30)
                logger.info(f"üìä API Response Status for page {page}: {response.status_code}")
                
                if response.status_code == 503:
                    if attempt < max_retries - 1:
                        logger.warning(f"‚è≥ API service overloaded (503), retrying in {retry_delay}s... (attempt {attempt + 1}/{max_retries})")
                        time.sleep(retry_delay)
                        retry_delay *= 2  # Exponential backoff
                        continue
                    else:
                        logger.error(f"‚ùå API still unavailable after {max_retries} attempts")
                break
            
            if response.status_code == 200:
                try:
                    json_data = response.json()
                    logger.info(f"üì® Response keys: {list(json_data.keys()) if isinstance(json_data, dict) else 'Not a dictionary'}")
                    
                    # Extract reviews from the response
                    page_reviews = []
                    
                    # Handle the actual API response structure
                    if isinstance(json_data, dict):
                        # The actual API returns: {"ProductName": "", "ProductRating": "", "Reviewers": [], ...}
                        reviewers = json_data.get("Reviewers", [])
                        logger.info(f"Found {len(reviewers)} reviewers in response")
                        
                        if reviewers and isinstance(reviewers, list):
                            for reviewer_data in reviewers:
                                if isinstance(reviewer_data, dict):
                                    # Extract review text from different possible fields
                                    review_text = None
                                    
                                    # Try different possible field names for review text
                                    for field in ["Review", "review", "ReviewText", "text", "comment", "ReviewContent"]:
                                        if field in reviewer_data and reviewer_data[field]:
                                            review_text = str(reviewer_data[field]).strip()
                                            break
                                    
                                    # If no review text found, try combining title and content
                                    if not review_text and "ReviewTitle" in reviewer_data and "ReviewContent" in reviewer_data:
                                        title = reviewer_data.get("ReviewTitle", "")
                                        content = reviewer_data.get("ReviewContent", "")
                                        if title or content:
                                            review_text = f"{title} {content}".strip()
                                    
                                    if review_text and len(review_text) > 10:
                                        page_reviews.append(review_text)
                                        logger.info(f"Added review: {review_text[:30]}...")
                    
                    all_reviews.extend(page_reviews)
                    logger.info(f"‚úÖ Found {len(page_reviews)} reviews on page {page}")
                    
                    # If no reviews found on this page, stop fetching
                    if not page_reviews:
                        logger.info(f"üõë No more reviews found, stopping at page {page}")
                        break
                        
                except Exception as e:
                    logger.error(f"‚ùå Failed to parse JSON for page {page}: {str(e)}")
                    logger.error(f"Raw response preview: {str(response.text)[:300]}...")
                    break
                    
            elif response.status_code == 429:
                logger.warning(f"‚è∞ Rate limit hit for page {page}")
                break
                
            else:
                logger.error(f"‚ùå API request failed for page {page} with status: {response.status_code}")
                logger.error(f"Response preview: {str(response.text)[:300]}...")
                break
                
        except requests.RequestException as e:
            logger.error(f"üåê Network error for page {page}: {str(e)}")
            break
        except Exception as e:
            logger.error(f"üí• Unexpected error for page {page}: {str(e)}")
            break
    
    logger.info(f"üéØ Total reviews fetched: {len(all_reviews)}")
    
    # If no reviews were fetched from API, return sample reviews for testing
    if not all_reviews:
        logger.warning("‚ö†Ô∏è No reviews found from API, using sample reviews")
        sample_reviews = [
            "This product is amazing! Great quality and fast shipping. Highly recommend!",
            "Not bad, but could be better. The quality is okay for the price.",
            "Terrible product! Broke after one day. Would not recommend to anyone.",
            "Excellent value for money. Exactly what I was looking for.",
            "Average product. Nothing special but does the job.",
            "The delivery was super quick but the product quality was disappointing.",
            "I've bought this product three times now. Always consistent quality.",
            "Overpriced for what it is. You can find better alternatives for less.",
            "The customer service was excellent when I had issues with my order.",
            "Beautiful design, but functionality could be improved.",
            "Love it! Exceeded my expectations in every way.",
            "The material feels cheap and flimsy. Not worth the money.",
            "Works as advertised. Satisfied with my purchase.",
            "Fantastic product! Will buy again.",
            "Did not meet my expectations. Returning it.",
            "Great for daily use and very durable.",
            "Highly recommend for anyone looking for quality and reliability.",
            "This product is amazing! Great quality and fast shipping. Highly recommend!",
            "I LOVEEE ITTTTT.....",
            "Just okay, nothing extraordinary.",
            "Exceeded my expectations in every way!",
            "Would not recommend to anyone.",
            "Fantastic product! Will buy again.",
            "Does the job",
            "no just no",
            "Scam",
            "This is not what i wanteeddddd"
        ]
        return sample_reviews
    
    # Remove duplicates while preserving order
    unique_reviews = []
    seen = set()
    for review in all_reviews:
        if review not in seen:
            unique_reviews.append(review)
            seen.add(review)
    
    logger.info(f"üîÑ Unique reviews after deduplication: {len(unique_reviews)}")
    
    # Limit to 50 reviews to avoid overwhelming the system
    final_reviews = unique_reviews[:50]
    logger.info(f"üìù Final reviews returned: {len(final_reviews)}")
    
    return final_reviews

def test_api_connection(asin: str = "B01H6GUCCQ") -> dict:
    """Test API connection with a known ASIN"""
    try:
        url = "https://real-time-amazon-data.p.rapidapi.com/product-reviews"
        querystring = {"asin": asin, "country": "US", "page": "1"}
        headers = {
            "x-rapidapi-key": settings.RAPIDAPI_KEY,
            "x-rapidapi-host": settings.RAPIDAPI_HOST
        }
        
        logger.debug(f"API Key: {settings.RAPIDAPI_KEY[:5]}...{settings.RAPIDAPI_KEY[-5:] if settings.RAPIDAPI_KEY else 'None'}")
        logger.debug(f"API Host: {settings.RAPIDAPI_HOST}")
        logger.debug(f"Testing API with ASIN: {asin}")
        
        response = requests.get(url, headers=headers, params=querystring, timeout=15)
        
        result = {
            "success": response.status_code == 200,
            "status_code": response.status_code,
            "response_preview": str(response.text)[:500] + "..." if len(response.text) > 500 else response.text
        }
        
        if response.status_code == 200:
            try:
                json_data = response.json()
                # Log the structure of the response
                result["response_keys"] = list(json_data.keys()) if isinstance(json_data, dict) else "Not a dictionary"
                if isinstance(json_data, dict) and "Reviewers" in json_data:
                    result["reviewers_count"] = len(json_data["Reviewers"])
                    result["product_name"] = json_data.get("ProductName", "N/A")
            except Exception as parse_error:
                result["parse_error"] = str(parse_error)
                
        return result
        
    except Exception as e:
        logger.error(f"API connection error: {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }